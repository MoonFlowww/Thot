1) issue from neural network size with Learning rate

Env:
model.set_optimizer(Thot::Optimizer::SGD(0.001f));
model.set_loss(Thot::Loss::MSE);
auto [x, y] = Thot::Data::generate_data(Thot::DataType::XOR, 2000, 0.0001, true);
model.train(x, y, Thot::Batch::Classic(32, 10), Thot::KFold::Classic(1), 1, true);
model.evaluate(x, y, Thot::Evaluation::Classification, true);

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Fully Connected | Linear | Ones~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
------------------------FC 3Layers------------------------
model.add(Thot::Layer::FC(2, 16, Thot::Activation::Linear, Thot::Initialization::Ones));
model.add(Thot::Layer::FC(16, 16,  Thot::Activation::Linear, Thot::Initialization::Ones));
model.add(Thot::Layer::FC(16, 1,  Thot::Activation::Linear, Thot::Initialization::Ones));

Result (Lr: 1e-3):
Epoch 0 - Average Loss: nan
Epoch 1 - Average Loss: nan
Epoch 2 - Average Loss: nan
Epoch 3 - Average Loss: nan
Epoch 4 - Average Loss: nan
Epoch 5 - Average Loss: nan
Epoch 6 - Average Loss: nan
Epoch 7 - Average Loss: nan
Epoch 8 - Average Loss: nan
Epoch 9 - Average Loss: nan

Fix Lr (1e-3) -> (1e-5):
Epoch 0 - Average Loss: 203.269
Epoch 1 - Average Loss: 0.28017
Epoch 2 - Average Loss: 0.270742
Epoch 3 - Average Loss: 0.265069
Epoch 4 - Average Loss: 0.25962
Epoch 5 - Average Loss: 0.254385
Epoch 6 - Average Loss: 0.249355
Epoch 7 - Average Loss: 0.244522
Epoch 8 - Average Loss: 0.239879
Epoch 9 - Average Loss: 0.235418


------------------------FC 5Layers------------------------
model.add(Thot::Layer::FC(2, 16, Thot::Activation::Linear, Thot::Initialization::Ones));
model.add(Thot::Layer::FC(16, 16,  Thot::Activation::Linear, Thot::Initialization::Ones));
model.add(Thot::Layer::FC(16, 16,  Thot::Activation::Linear, Thot::Initialization::Ones));
model.add(Thot::Layer::FC(16, 16,  Thot::Activation::Linear, Thot::Initialization::Ones));
model.add(Thot::Layer::FC(16, 1,  Thot::Activation::Linear, Thot::Initialization::Ones));


Result (Lr: 1e-3):
Epoch 0 - Average Loss: nan
Epoch 1 - Average Loss: nan
Epoch 2 - Average Loss: nan
Epoch 3 - Average Loss: nan
Epoch 4 - Average Loss: nan
Epoch 5 - Average Loss: nan
Epoch 6 - Average Loss: nan
Epoch 7 - Average Loss: nan
Epoch 8 - Average Loss: nan
Epoch 9 - Average Loss: nan

Fix Lr (1e-3) -> (1e-8):
Epoch 0 - Average Loss: 261685
Epoch 1 - Average Loss: 0.264043
Epoch 2 - Average Loss: 0.264038
Epoch 3 - Average Loss: 0.264033
Epoch 4 - Average Loss: 0.264028
Epoch 5 - Average Loss: 0.264023
Epoch 6 - Average Loss: 0.264018
Epoch 7 - Average Loss: 0.264013
Epoch 8 - Average Loss: 0.264008
Epoch 9 - Average Loss: 0.264003





IDEA:
- Backward explosion
    -Gradients through linear layers with w=1 also scale multiplicatively with fan-in.
    -For 16 units, backward pass multiplies gradient norms by ~16 per layer.
    -With 5 layers, thatâ€™s ~16^5 amplification.