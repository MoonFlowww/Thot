TODO


Model Technologies
	Learning
	- RL
	- IRL

	Transformers
	- classic
	- Titan
	- Atlas

	Module
	- MoE
	- MoRE

	Layers
	- Reservoirs
	- Dilated Convolution
	- Pooling
	- Pool of Markov
	- Pool of HM
	- Graphs

	Blocks
	- TCB
	
	Norm-Layers
	- RMSE
	- DyT
	- PreLN
	- Batch Norm

	Attentions
	- Multi head attention
	- Multi head latent attention
	- Flash attention
	- Causal attention
	- SE Block
	- 2Simplicials
	- Multi-Channel Att

	Attentions Technologies
	- RoPE
	- KV cache

	Memory
	- LSTM
	- xLSTM
	- rLSTM
	- sLSTM


	
Pre-Training

	Normalization
	- Zscore
	- Uscore
	- DePre
	- MinMax
	- Log

	Transformation
	- FFT
	- WT
	- SWT
	- Elher loops

	Dim reduction/Feature Selection
	- PCA
	- UMAP
	- t-sne
		+ Partial Dependence




Training

	Data during Training:
	- kfold
	- timeserie kfold
	- Bayesian Active Learning
	- LWoL (home made BAL)
	- Curriculum

	Penalizations:
	- LSD stable-rank
	- dropout
	- L2
	- Informed Mechanism

	Data
	- Lorenz 74
	- API to hugging face
	- MNIST
	- csv reader
	- SQL collector

	Loss
	- Meta Learning
	- Time-flow loss (SUNDIAL)

	Others
	- Path Shadowing MC



Post-Training
	Metrics
	- ADF
	- Ljung box
	- JS/Bhattacharyya... Div
	- SETOL
	- Entropy (Aleatory/Epistemic)
	- VPT (model vs input dy) SCVAR (SCvariance)
	- LTA/LTU
	- SHAP
	- Average Mutual Informations







